{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcNZPZWyOEtc"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating environment\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "num_states = env.observation_space.n\n",
        "row = 5\n",
        "col = 5\n",
        "\n",
        "# Actions\n",
        "SOUTH = 0\n",
        "NORTH = 1\n",
        "EAST = 2\n",
        "WEST = 3\n",
        "PICK = 4\n",
        "DROP = 5\n",
        "total_actions = [SOUTH, NORTH, EAST, WEST, PICK, DROP]\n",
        "primitive_actions= [SOUTH, NORTH, EAST, WEST]\n",
        "num_pacts = len(primitive_actions)\n",
        "\n",
        "# Primary Destinations for Pickup and Drop\n",
        "RED = 0\n",
        "GREEN = 1\n",
        "YELLOW = 2\n",
        "BLUE = 3\n",
        "IN_TAXI = 4\n",
        "\n",
        "passenger_locs = [RED, GREEN, YELLOW, BLUE, IN_TAXI]\n",
        "destinations = [RED, GREEN, YELLOW, BLUE]\n",
        "destination_coords = [[0,0], [0,4], [4,0], [4,3]]\n",
        "\n",
        "num_plocs = len(passenger_locs)\n",
        "num_dlocs = len(destinations)\n",
        "\n",
        "# cur_state = ((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination"
      ],
      "metadata": {
        "id": "YOO_KG8wOK8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon-greedy policy for options and actions\n",
        "\n",
        "def choose_option_eg(qvals, num_options, eps):\n",
        "    '''\n",
        "    Choosing the option for the current state\n",
        "    based on an exploration-exploitation trade-off\n",
        "    using epsilon-greedy policy\n",
        "    '''\n",
        "    if np.random.rand() < eps:\n",
        "        # Exploration\n",
        "        option = np.random.choice(num_options)\n",
        "\n",
        "    else:\n",
        "        # Exploitation\n",
        "        option = np.argmax(qvals)\n",
        "\n",
        "    return option\n",
        "\n",
        "def choose_action_eg(qvals, num_pacts, eps):\n",
        "    '''\n",
        "    Choosing the action for the current state\n",
        "    based on an exploration-exploitation trade-off\n",
        "    using epsilon-greedy policy\n",
        "    '''\n",
        "    if np.random.rand() < eps:\n",
        "        # Exploration\n",
        "        action = np.random.choice(num_pacts)\n",
        "\n",
        "    else:\n",
        "        # Exploitation\n",
        "        action = np.argmax(qvals)\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "3Edbuw-ZOgf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_passenger_in_taxi(ploc):\n",
        "    '''\n",
        "    Checking if passenger in taxi\n",
        "    '''\n",
        "    if ploc == 4:\n",
        "        ptaxi = True\n",
        "    else:\n",
        "        ptaxi = False\n",
        "    return ptaxi"
      ],
      "metadata": {
        "id": "vY6Cvzz-OxsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Options\n",
        "\n",
        "class Option:\n",
        "    def __init__(self, desgn_loc, null_act):\n",
        "        self.goal_loc = destination_coords[desgn_loc]\n",
        "        self.goal_null_act = null_act\n",
        "        self.qvals = np.zeros((row, col, num_pacts))\n",
        "        self.sa_freq = np.zeros((row, col, num_pacts))\n",
        "\n",
        "    def reset(self):\n",
        "        self.qvals = np.zeros((row, col, num_pacts))\n",
        "        self.sa_freq = np.zeros((row, col, num_pacts))\n",
        "\n",
        "    def execute(self, state, ploc, dloc, ptaxi, eps):\n",
        "        '''\n",
        "        Choosing primitive action within the option\n",
        "        and determining option termination\n",
        "\n",
        "        '''\n",
        "\n",
        "        dstate = destination_coords[dloc]\n",
        "        optdone = False\n",
        "\n",
        "        if state[0] == self.goal_loc[0] and state[1] == self.goal_loc[1]:\n",
        "            optdone = True\n",
        "\n",
        "            if not ptaxi:\n",
        "                pstate = destination_coords[ploc]\n",
        "                if pstate[0] == state[0] and pstate[1] == state[1]:\n",
        "                    # Picking up passenger\n",
        "                    optact = PICK\n",
        "                else:\n",
        "                    # Terminating the option requires taxi to stay in same state for stable update\n",
        "                    optact = self.goal_null_act\n",
        "            if ptaxi:\n",
        "                if dstate[0] == state[0] and dstate[1] == state[1]:\n",
        "                    # Dropping passenger\n",
        "                    optact = DROP\n",
        "                else:\n",
        "                    # Terminating the option requires taxi to stay in same state for stable update\n",
        "                    optact = self.goal_null_act\n",
        "        else:\n",
        "            # Choose next action\n",
        "            optact = choose_action_eg(self.qvals[state[0], state[1]], num_pacts, eps)\n",
        "\n",
        "\n",
        "        return optact, optdone"
      ],
      "metadata": {
        "id": "BSK-l8W5OzWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning parameters\n",
        "num_episodes = 5000\n",
        "num_runs = 5\n",
        "gamma = 0.9\n",
        "alpha = 0.1\n",
        "\n",
        "# Epsilon greedy\n",
        "eps_decay = 0.99\n",
        "eps_start = 0.5\n",
        "eps_final = 0.01"
      ],
      "metadata": {
        "id": "5ruQYL3mRVFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training taxi\n",
        "\n",
        "run_returns = []\n",
        "run_qo = []\n",
        "run_options = []\n",
        "\n",
        "for run in range(num_runs):\n",
        "    print(f'Beginning Run {run+1}')\n",
        "\n",
        "    # Creating Options\n",
        "    Red = Option(RED, 1)\n",
        "    Green = Option(GREEN, 1)\n",
        "    Yellow = Option(YELLOW, 0)\n",
        "    Blue = Option(BLUE, 0)\n",
        "\n",
        "    options = [Red, Green, Yellow, Blue]\n",
        "    option_names = ['Red', 'Green', 'Yellow', 'Blue']\n",
        "    num_options = len(options)\n",
        "    q_o = np.zeros((num_plocs*num_dlocs, num_options))\n",
        "\n",
        "    eps = eps_start\n",
        "\n",
        "    ep_rew = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        # print(f'\\n Episode {ep} begins!\\n')\n",
        "        episode_reward = 0\n",
        "\n",
        "        # Initial state\n",
        "        state = env.reset()\n",
        "        tx, ty, ploc, dloc = list(env.decode(state))\n",
        "        t_coord = [tx, ty]\n",
        "\n",
        "        # Checking if passenger is in taxi\n",
        "        ptaxi = check_passenger_in_taxi(ploc)\n",
        "\n",
        "        done = False # Episode flag\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # Get option using ep-greedy\n",
        "            opt_state = num_dlocs*ploc + dloc\n",
        "            opt_id = choose_option_eg(q_o[opt_state], num_options, eps)\n",
        "\n",
        "            opt = options[opt_id]\n",
        "            optdone = False # Option flag\n",
        "\n",
        "            start_state = state\n",
        "\n",
        "            reward_bar = 0\n",
        "            steps = 0\n",
        "\n",
        "            # Execute option till termination\n",
        "            while not optdone:\n",
        "                ptaxi = check_passenger_in_taxi(ploc)\n",
        "\n",
        "                # Get primitive action, optact and optdone\n",
        "                optact, optdone = opt.execute([tx, ty], ploc, dloc, ptaxi, eps)\n",
        "\n",
        "                # Get next state, reward, done(episode), info(term prob, action mask)\n",
        "                next_state, reward, done, info = env.step(optact)\n",
        "\n",
        "                tnx, tny, pnloc, dnloc = list(env.decode(next_state))\n",
        "                tn_coord = [tnx, tny]\n",
        "                opt_ns = num_dlocs*pnloc + dnloc\n",
        "                # print([tx,ty], [tnx,tny], reward, optdone, done, ploc, dloc, pnloc, dnloc)\n",
        "                reward_bar = gamma*reward_bar + reward\n",
        "                steps += 1\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Update primitive actions\n",
        "                if optact < 4:\n",
        "                    opt.qvals[tx, ty, optact] = opt.qvals[tx, ty, optact] + alpha * (reward + gamma*np.max(opt.qvals[tnx, tny]) - opt.qvals[tx, ty, optact])\n",
        "                    opt.sa_freq[tx, ty, optact] += 1\n",
        "\n",
        "                    if optdone:\n",
        "                        opt.sa_freq[tnx, tny, optact] += 1\n",
        "\n",
        "\n",
        "                state = next_state\n",
        "                tx, ty, ploc, dloc = list(env.decode(state))\n",
        "                t_coord = [tx, ty]\n",
        "                opt_state = opt_ns\n",
        "\n",
        "            tx_, ty_, ploc_, dloc_ = list(env.decode(start_state))\n",
        "            opt_start_state = num_dlocs*ploc_ + dloc_\n",
        "\n",
        "            # Update option\n",
        "            q_o[opt_start_state, opt_id] = q_o[opt_start_state, opt_id] + alpha * ((reward_bar + ((gamma**steps) * np.max(q_o[opt_ns,:]))) - q_o[opt_start_state, opt_id])\n",
        "\n",
        "        eps = max(eps_final, eps_decay*eps)\n",
        "        ep_rew.append(episode_reward)\n",
        "        rew_avg100 = [np.average(ep_rew[i:i+100]) for i in range(len(ep_rew)-100)]\n",
        "\n",
        "    print(f'Run: {run+1} - Max reward: {max(ep_rew)}')\n",
        "    run_returns.append(ep_rew)\n",
        "    run_qo.append(q_o)\n",
        "    run_options.append([Red, Green, Yellow, Blue])"
      ],
      "metadata": {
        "id": "ZG-_9oI7RXEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Episodic Returns averaged over 5 runs\n",
        "\n",
        "returns_mean = np.mean(run_returns, 0)\n",
        "plt.plot(returns_mean)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episodic Returns')\n",
        "plt.title('SMDP Q-Learning (Episodic Returns averaged over 5 runs)')"
      ],
      "metadata": {
        "id": "XwCrfwuedwbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Maximum return over 5 runs {max(returns_mean)}')"
      ],
      "metadata": {
        "id": "JzYdsB1wjslV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Episodic Returns averaged over 5 runs - 400th-5000th episode to visualize exact reward values\n",
        "xvals = range(400, 5000)\n",
        "plt.plot(xvals, returns_mean[400:5000])\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episodic Returns')\n",
        "plt.title(f'SMDP Q-Learning (Episodic Returns averaged over 5 runs)\\n400th-5000th episode to visualize exact reward values')"
      ],
      "metadata": {
        "id": "b3xtReGmhkEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaging returns every 100 episodes (over 5 runs)\n",
        "return_avg100 = [np.average(returns_mean[i:i+100]) for i in range(len(returns_mean)-100)]"
      ],
      "metadata": {
        "id": "5U9QoAcSeG_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Maximum return over 5 runs (returns averaged every 100 episodes in each run) {max(return_avg100)}')"
      ],
      "metadata": {
        "id": "fJI2hEBUiRQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Episodic Returns averaged over 5 runs - returns in every run averaged every 100 episodes\n",
        "plt.plot(return_avg100)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episodic Returns (averaged every 100 episodes)')\n",
        "plt.title('SMDP Q-Learning (Episodic Returns averaged over 5 runs)')"
      ],
      "metadata": {
        "id": "URVgdJwqh8kV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Episodic Returns averaged over 5 runs - returns in every run averaged every 100 episodes\n",
        "# From 400th episode to visualize exact reward values\n",
        "xvals = range(400,4900)\n",
        "plt.plot(xvals, return_avg100[400:4900])\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Episodic Returns (averaged every 100 episodes)')\n",
        "plt.title(f'SMDP Q-Learning (Episodic Returns averaged over 5 runs)\\nFrom 400th episode to visualize exact reward values')"
      ],
      "metadata": {
        "id": "FQOQg7YDiKLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Q values on heatmap\n",
        "\n",
        "def plotQvals(Q, message = \"Q plot\"):\n",
        "    # Visualize Q values across taxi env\n",
        "    Q = np.flip(Q, 0)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(message)\n",
        "    plt.pcolor(Q.max(-1), edgecolors='k', linewidths=2)\n",
        "    plt.colorbar()\n",
        "    def x_direct(a):\n",
        "        if a in [NORTH, SOUTH]:\n",
        "            return 0\n",
        "        return 1 if a == EAST else -1\n",
        "    def y_direct(a):\n",
        "        if a in [EAST, WEST]:\n",
        "            return 0\n",
        "        return 1 if a == NORTH else -1\n",
        "    policy = Q.argmax(-1)\n",
        "    policyx = np.vectorize(x_direct)(policy)\n",
        "    policyy = np.vectorize(y_direct)(policy)\n",
        "    idx = np.indices(policy.shape)\n",
        "    plt.quiver(idx[1].ravel()+0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QssMIrN8YnxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Q values of option policies - Red, Green, Yellow, Blue\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6), dpi=300)\n",
        "# fig.title('Q value visualization for the Option policies')\n",
        "\n",
        "f1 = plotQvals(Red.qvals, 'Learnt Policy of Option Red')\n",
        "f2 = plotQvals(Green.qvals, 'Learnt Policy of Option Green')\n",
        "f3 = plotQvals(Yellow.qvals, 'Learnt Policy of Option Yellow')\n",
        "f4 = plotQvals(Blue.qvals, 'Learnt Policy of Option Blue')"
      ],
      "metadata": {
        "id": "at3-p-RRkk-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize state-action frequencies on heatmap\n",
        "\n",
        "def plot_state_visit(state_visits, message = 'Mean Number of State Visits over 5 runs (5000 episodes each)'):\n",
        "    # Visualize number of times agent visits each state\n",
        "    state_visits = np.flip(state_visits, 0)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.title(message)\n",
        "    plt.pcolor(state_visits.max(-1), edgecolors='k', linewidths=2, cmap=\"GnBu\")\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2F1FhLx2094L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing state-action frequencies of option policies - Red, Green, Yellow, Blue\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6), dpi=300)\n",
        "# fig.title('Q value visualization for the Option policies')\n",
        "\n",
        "f1 = plot_state_visit(Red.sa_freq, 'State-Action Frequencies of Learnt Policy of Option Red')\n",
        "f2 = plot_state_visit(Green.sa_freq, 'State-Action Frequencies of Learnt Policy of Option Green')\n",
        "f3 = plot_state_visit(Yellow.sa_freq, 'State-Action Frequencies of Learnt Policy of Option Yellow')\n",
        "f4 = plot_state_visit(Blue.sa_freq, 'State-Action Frequencies of Learnt Policy of Option Blue')"
      ],
      "metadata": {
        "id": "p2ah_-iD9Zt3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}